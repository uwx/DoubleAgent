<?xml version="1.0" encoding="utf-8"?>
<topic id="e0846715-419c-11e1-9d4f-002421116fb2" revisionNumber="1">
	<developerConceptualDocument
	  xmlns="http://ddue.schemas.microsoft.com/authoring/2003/5"
	  xmlns:xlink="http://www.w3.org/1999/xlink">

		<introduction>
			<para>
				In addition to supporting mouse and keyboard interaction, Double Agent includes direct support for speech
				input. Because Double Agent&#39;s support for speech input is based on the
				<token>SAPIname</token> (<token>SAPI</token>), you can use Double Agent with speech recognition engines that include
				the <token>SAPI</token>-required support.
			</para>
			<para>
				To support speech input, you define a <legacyItalic>grammar</legacyItalic>, a set of words you want the
				speech recognition engine to listen and match for as the <unmanagedCodeEntityReference>VoiceGrammar</unmanagedCodeEntityReference> setting
				for a <unmanagedCodeEntityReference>Command</unmanagedCodeEntityReference> in your <unmanagedCodeEntityReference>Commands</unmanagedCodeEntityReference> collection. You can
				include optional and alternative words and repeated sequences in your <legacyItalic>grammar</legacyItalic>.
			</para>
			<autoOutline/>
		</introduction>

		<section address="Listen">
			<title>
				<token>ListeningModeUc</token>
			</title>
			<content>
				<para>
					The user can initiate speech input by pressing and holding the push-to-talk
					<token>ListeningKey</token>. In this <token>ListeningMode</token>, if the speech engine receives the beginning
					of spoken input, it holds the audio channel open until it detects the end of the utterance. However,
					when not receiving input, it does not block audio output. This enables the user to issue multiple voice
					commands while holding down the key, and the character can respond when the user isn't speaking.
				</para>
				<para>
					The <token>ListeningMode</token> times out once the user releases the <token>ListeningKey</token>.
					The user can adjust the time-out for this mode using the <link xlink:href="e0846716-419c-11e1-9d4f-002421116fb2#PropSheet">
						<token>AdvancedCharacterOptions</token>
					</link> window. You cannot set this time-out from your client application code.
				</para>
				<para>
					If a character attempts to speak while the user is speaking, the character's audible output fails though
					text may still be displayed in its <link xlink:href="e0846713-419c-11e1-9d4f-002421116fb2#Balloon">
						<token>WordBalloon</token>
					</link>. If
					the character has the audio channel while the <token>ListeningKey</token> is pressed, the
					server automatically transfers control back to the user after processing the text in the
					<unmanagedCodeEntityReference>Speak</unmanagedCodeEntityReference> method. An optional tone is played to cue the user to begin speaking.
					This allows the
					user to provide input even if the application driving the character failed to provide logical pauses
					in its output.
				</para>
				<para>
					You can also use the <unmanagedCodeEntityReference>Listen</unmanagedCodeEntityReference> method to initiate speech input. Calling this method
					turns on the speech recognition for a pre-defined period of time. If there is no input during this interval,
					Double Agent automatically turns off the speech recognition engine and frees up the audio channel. This
					avoids blocking input to or output from the audio device and minimizes the processor overhead the speech
					recognition uses when it is on. You can also use the <unmanagedCodeEntityReference>Listen</unmanagedCodeEntityReference> method to turn
					off speech input. However, be aware that because the speech recognition engine operates asynchronously,
					the effect may not be immediate. As a result, it is possible to receive a <unmanagedCodeEntityReference>Command</unmanagedCodeEntityReference>
					event even <legacyItalic>after</legacyItalic> your code calls <unmanagedCodeEntityReference>Listen</unmanagedCodeEntityReference> to turn off speech input.
				</para>
				<para>
					Whether the user presses the <token>ListeningKey</token> or your client application calls
					the <unmanagedCodeEntityReference>Listen</unmanagedCodeEntityReference> method to initiate speech input, the speech recognition engine attempts
					to match a user utterance to the grammar for the commands that have been defined, and passes the information
					back to Double Agent. Double Agent then notifies the client application using the <unmanagedCodeEntityReference>Command</unmanagedCodeEntityReference>
					event, passing back the <unmanagedCodeEntityReference>UserInput</unmanagedCodeEntityReference> object that identifies of the best matching
					<unmanagedCodeEntityReference>Command</unmanagedCodeEntityReference> and next two alternative matches (if any), a confidence score, and
					the matching text for each match.
				</para>
				<para>
					Double Agent also notifies your client application when it matches the speech input to one of its pre-defined
					commands. While the <unmanagedCodeEntityReference>CommandID</unmanagedCodeEntityReference> is empty, you still get the confidence score
					and text matched. When in <token>ListeningMode</token>, the server automatically plays the
					animation assigned to the character's <legacyItalic>Listening</legacyItalic> state. Then, when an utterance
					is actually detected, the server plays the character's <legacyItalic>Hearing</legacyItalic> state animation.
					The server will keep the character in an attentive state until the utterance has ended. This provides
					the appropriate social feedback to cue the user for input.
				</para>
				<para>
					If the user disables speech input in the <link xlink:href="e0846716-419c-11e1-9d4f-002421116fb2#PropSheet">
						<token>AdvancedCharacterOptions</token>
					</link>
					window, the <token>ListeningKey</token> will also be disabled. Similarly, attempting to
					call the <unmanagedCodeEntityReference>Listen</unmanagedCodeEntityReference> method when speech input is disabled will cause the method
					to fail. Also note that Double Agent does not enable the <token>ListeningKey</token> until
					one of its clients has successfully loaded a speech engine or has authored a <unmanagedCodeEntityReference>VoiceGrammar</unmanagedCodeEntityReference>
					for one of its <unmanagedCodeEntityReference>Command</unmanagedCodeEntityReference> objects.
				</para>
			</content>
		</section>

		<section address="Engine">
			<title>Speech Engine Selection</title>
			<content>
				<para>
					A character's <unmanagedCodeEntityReference>LanguageID</unmanagedCodeEntityReference> setting determines its default speech input language;
					Double Agent requests <token>SAPI</token> for an installed engine that matches that language. If a client application
					does not specify a language preference, Double Agent will attempt to find a speech recognition engine
					that matches the user default language. If no engine is available matching this language, speech input
					is disabled for that character.
				</para>
				<para>
					You can also request a specific speech recognition engine by specifying the character's
					<unmanagedCodeEntityReference>SRModeID</unmanagedCodeEntityReference> property.
					However, if the <unmanagedCodeEntityReference>LanguageID</unmanagedCodeEntityReference>
					for that <unmanagedCodeEntityReference>SRModeID</unmanagedCodeEntityReference> does not match the client's language setting,
					the call will fail (raise an error in
					the control). The speech recognition engine will then remain the last successfully set engine by the
					client, or if none, the engine that matches the current system language. If there is still no match,
					speech input is not available for that client.
				</para>
				<para>
					Double Agent automatically loads a speech recognition engine when speech input is initiated by a user
					pressing the <token>ListeningKey</token> or the <link xlink:href="e0846714-419c-11e1-9d4f-002421116fb2#Active">
						<token>InputActive</token>
					</link>
					client calls the <unmanagedCodeEntityReference>Listen</unmanagedCodeEntityReference> method. However, an engine may also
					be loaded when setting or querying its <unmanagedCodeEntityReference>SRModeID</unmanagedCodeEntityReference>, setting or querying the properties
					of the <link xlink:href="e0846717-419c-11e1-9d4f-002421116fb2">
						<token>VoiceCommands</token>
					</link> window, querying <unmanagedCodeEntityReference>ListeningStatus</unmanagedCodeEntityReference>,
					or when speech is enabled and the user displays the <link xlink:href="e08466be-419c-11e1-9d4f-002421116fb2#SpeechInput">
						<token>InputPage</token>
					</link>
					of the <link xlink:href="e0846716-419c-11e1-9d4f-002421116fb2#PropSheet">
						<token>AdvancedCharacterOptions</token>
					</link> window.
					However, Double Agent only keeps loaded the speech engines that clients are using.
				</para>
			</content>
		</section>

		<section address="Events">
			<title>Speech Input Events</title>
			<content>
				<para>
					In addition, to the <unmanagedCodeEntityReference>Command</unmanagedCodeEntityReference> event notification, Double Agent also notifies the
					<link xlink:href="e0846714-419c-11e1-9d4f-002421116fb2#Active">
						<token>InputActive</token>
					</link> client when Double Agent turns the
					<token>ListeningMode</token> on or off, with the <unmanagedCodeEntityReference>ListenStart</unmanagedCodeEntityReference> and <unmanagedCodeEntityReference>ListenComplete</unmanagedCodeEntityReference>
					events. However, if the user presses the <token>ListeningKey</token> and there is no matching
					speech recognition engine available for the topmost character of the
					<link xlink:href="e0846714-419c-11e1-9d4f-002421116fb2#Active">
						<token>InputActive</token>
					</link> client, Double Agent starts the <token>ListeningKey</token> time-out, but
					does not generate a <unmanagedCodeEntityReference>ListenStart</unmanagedCodeEntityReference> event for the active client of the character.
					If, before the time-out completes, the user activates another character with speech recognition engine
					support, Double Agent attempts to activate speech input and generates the <unmanagedCodeEntityReference>ListenStart</unmanagedCodeEntityReference>
					event.
				</para>
				<para>
					Similarly, if a client application attempts to turn on the <token>ListeningMode</token> using
					the <unmanagedCodeEntityReference>Listen</unmanagedCodeEntityReference> method and there is no matching speech recognition engine available,
					the call fails and Double Agent does not generate a <unmanagedCodeEntityReference>ListenStart</unmanagedCodeEntityReference> event. In the
					Double Agent control, the <unmanagedCodeEntityReference>Listen</unmanagedCodeEntityReference> method returns <literal>False</literal>,
					but the call does not raise an error.
				</para>
				<para>
					When the <token>ListeningMode</token> is active and the user switches to a character that
					uses a different speech recognition engine, Double Agent switches to and activates that engine and triggers
					a <unmanagedCodeEntityReference>ListenComplete</unmanagedCodeEntityReference> and then a <unmanagedCodeEntityReference>ListenStart</unmanagedCodeEntityReference> event. If
					the activated character does not have an available speech recognition engine (because one is not installed
					or none match the activated character's <unmanagedCodeEntityReference>LanguageID</unmanagedCodeEntityReference> setting), Double Agent will
					trigger the <unmanagedCodeEntityReference>ListenComplete</unmanagedCodeEntityReference> event for the previously activated character and
					passes back a value in the <parameterReference>Cause</parameterReference> parameter. However, Double Agent does not generate
					<unmanagedCodeEntityReference>ListenStart</unmanagedCodeEntityReference> or <unmanagedCodeEntityReference>ListenComplete</unmanagedCodeEntityReference> events for the clients
					that do not have speech recognition support.
				</para>
				<para>
					If a client successfully calls the <unmanagedCodeEntityReference>Listen</unmanagedCodeEntityReference> method and a character without speech
					recognition engine support becomes <link xlink:href="e0846714-419c-11e1-9d4f-002421116fb2#Active">
						<token>InputActive</token>
					</link> before
					the <token>ListeningMode</token> time-out completes, and then the user switches back to
					the character of the original client, Double Agent will generate a <unmanagedCodeEntityReference>ListenStart</unmanagedCodeEntityReference>
					event for that client.
				</para>
				<para>
					If the <link xlink:href="e0846714-419c-11e1-9d4f-002421116fb2#Active">
						<token>InputActive</token>
					</link> client switches speech recognition
					engines by changing <unmanagedCodeEntityReference>SRModeID</unmanagedCodeEntityReference> while in <token>ListeningMode</token>,
					Double Agent switches to and activates that engine without re-triggering the <unmanagedCodeEntityReference>ListenStart</unmanagedCodeEntityReference>
					event. However, if the specified engine is not available, the call fails (raises an error) and Double
					Agent also calls the <unmanagedCodeEntityReference>ListenComplete</unmanagedCodeEntityReference> event.
				</para>
			</content>
		</section>

		<relatedTopics>
		</relatedTopics>

	</developerConceptualDocument>
</topic>
