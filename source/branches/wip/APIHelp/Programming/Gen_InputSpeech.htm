<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
	<title>Speech Input Support</title>
	<link rel="STYLESHEET" href="../css/dahelp.css">
</head>
<body>
	<table class="pagetable" cellspacing="0" cellpadding="0">
		<tr>
			<td class="runninghead">Double&nbsp;Agent&nbsp;API</td>
		</tr>
		<tr>
			<td class="pagehead">
				<h3>Speech Input Support</h3>
			</td>
		</tr>
		<tr>
			<td class="page">
				<p>In addition to supporting mouse and keyboard interaction, Double Agent includes direct support for speech
					input. Because Double Agent&#39;s support for speech input is based on Microsoft<sup>®</sup> SAPI (Speech
					Application Programming Interface), you can use Double Agent with speech recognition engines that include
					the SAPI-required support.</p>
				<p>To support speech input, you define a <em class="glossary">grammar</em>, a set of words you want the
					speech recognition engine to listen and match for as the <em class="keyword">VoiceGrammar</em> setting
					for a <em class="keyword">Command</em> in your <em class="keyword">Commands</em> collection. You can
					include optional and alternative words and repeated sequences in your <em class="glossary">grammar</em>.</p>
				<dl>
					<dt>
						<h4><a name="Listen"></a>Listening Mode</h4>
					</dt>
					<dd>
						<p>The user can initiate speech input by pressing and holding the push-to-talk <em class="glossary">Listening
							key</em>. In this <em class="glossary">Listening mode</em>, if the speech engine receives the beginning
							of spoken input, it holds the audio channel open until it detects the end of the utterance. However,
							when not receiving input, it does not block audio output. This enables the user to issue multiple voice
							commands while holding down the key, and the character can respond when the user isn't speaking.</p>
						<p>The <em class="glossary">Listening mode</em> times out once the user releases the <em class="glossary">
							Listening key</em>. The user can adjust the time-out for this mode using the <a href="Gen_Config.htm#PropSheet">
							Advanced Character Options</a> window. You cannot set this time-out from your client application code.</p>
						<p>If a character attempts to speak while the user is speaking, the character's audible output fails though
							text may still be displayed in its word balloon. If the character has the audio channel while the <em
								class="glossary">Listening key</em> is pressed, the server automatically transfers control back
							to the user after processing the text in the <em class="keyword">Speak</em> method. An optional tone
							is played to cue the user to begin speaking. This allows the user to provide input even if the application
							driving the character failed to provide logical pauses in its output.</p>
						<p>You can also use the <em class="keyword">Listen</em> method to initiate speech input. Calling this method
							turns on the speech recognition for a pre-defined period of time. If there is no input during this interval,
							Double Agent automatically turns off the speech recognition engine and frees up the audio channel. This
							avoids blocking input to or output from the audio device and minimizes the processor overhead the speech
							recognition uses when it is on. You can also use the <em class="keyword">Listen</em> method to turn
							off speech input. However, be aware that because the speech recognition engine operates asynchronously,
							the effect may not be immediate. As a result, it is possible to receive a <em class="keyword">Command</em>
							event even <em>after</em> your code calls <em class="keyword">Listen</em> to turn off speech input.</p>
						<p>Whether the user presses the <em class="glossary">Listening key</em> or your client application calls
							the <em class="keyword">Listen</em> method to initiate speech input, the speech recognition engine attempts
							to match a user utterance to the grammar for the commands that have been defined, and passes the information
							back to Double Agent. Double Agent then notifies the client application using the <em class="keyword">Command</em>
							event, passing back the <em class="keyword">UserInput</em> object that identifies of the best matching
							<em class="keyword">Command</em> and next two alternative matches (if any), a confidence score, and
							the matching text for each match.</p>
						<p>Double Agent also notifies your client application when it matches the speech input to one of its pre-defined
							commands. While the <em class="keyword">CommandID</em> is empty, you still get the confidence score
							and text matched. When in <em class="glossary">Listening mode</em>, the server automatically plays the
							animation assigned to the character's <em class="glossary">Listening</em> state. Then, when an utterance
							is actually detected, the server plays the character's <em class="glossary">Hearing</em> state animation.
							The server will keep the character in an attentive state until the utterance has ended. This provides
							the appropriate social feedback to cue the user for input.</p>
						<p>If the user disables speech input in the <a href="Gen_Config.htm#PropSheet">Advanced Character Options</a>
							window, the <em class="glossary">Listening key</em> will also be disabled. Similarly, attempting to
							call the <em class="keyword">Listen</em> method when speech input is disabled will cause the method
							to fail. Also note that Double Agent does not enable the <em class="glossary">Listening key</em> until
							one of its clients has successfully loaded a speech engine or has authored a <em class="keyword">VoiceGrammar</em>
							for one of its <em class="keyword">Command</em> objects.</p>
					</dd>
					<dt>
						<h4><a name="Engine"></a>Speech Engine Selection</h4>
					</dt>
					<dd>
						<p>A character's <em class="keyword">LanguageID</em> setting determines its default speech input language;
							Double Agent requests SAPI for an installed engine that matches that language. If a client application
							does not specify a language preference, Double Agent will attempt to find a speech recognition engine
							that matches the user default language. If no engine is available matching this language, speech input
							is disabled for that character.</p>
						<p>You can also request a specific speech recognition engine by specifying the character's <em class="keyword">
							SRModeID</em> property. However, if the <em class="keyword">LanguageID</em> for that <em class="keyword">
								SRModeID</em> does not match the client's language setting, the call will fail (raise an error in
							the control). The speech recognition engine will then remain the last successfully set engine by the
							client, or if none, the engine that matches the current system language. If there is still no match,
							speech input is not available for that client.</p>
						<p>Double Agent automatically loads a speech recognition engine when speech input is initiated by a user
							pressing the <em class="glossary">Listening key</em> or the <a class="see" href="Gen_Input.htm#Active">
							input-active</a> client calls the <em class="keyword">Listen</em> method. However, an engine may also
							be loaded when setting or querying its <em class="keyword">SRModeID</em>, setting or querying the properties
							of the <a href="Gen_VoiceCommands.htm">Voice Commands</a> window, querying <em class="keyword">ListeningStatus</em>,
							or when speech is enabled and the user displays the <a href="../html/UI_PropSheet.htm#SpeechInput">Speech Input</a> page of the
							<a href="../html/UI_PropSheet.htm">Advanced Character Options</a> window. However, Double Agent only
							keeps loaded the speech engines that clients are using.</p>
					</dd>
					<dt>
						<h4><a name="Events"></a>Speech Input Events</h4>
					</dt>
					<dd>
						<p>In addition, to the <em class="keyword">Command</em> event notification, Double Agent also notifies the
							<a class="see" href="Gen_Input.htm#Active">input-active</a> client when Double Agent turns the <em class="glossary">
								Listening mode</em> on or off, with the <em class="keyword">ListenStart</em> and <em class="keyword">ListenComplete</em>
							events. However, if the user presses the <em class="glossary">Listening key</em> and there is no matching
							speech recognition engine available for the topmost character of the <a class="see" href="Gen_Input.htm#Active">
							input-active</a> client, Double Agent starts the <em class="glossary">Listening key</em> time-out, but
							does not generate a <em class="keyword">ListenStart</em> event for the active client of the character.
							If, before the time-out completes, the user activates another character with speech recognition engine
							support, Double Agent attempts to activate speech input and generates the <em class="keyword">ListenStart</em>
							event.</p>
						<p>Similarly, if a client application attempts to turn on the <em class="glossary">Listening mode</em> using
							the <em class="keyword">Listen</em> method and there is no matching speech recognition engine available,
							the call fails and Double Agent does not generate a <em class="keyword">ListenStart</em> event. In the
							Double Agent control, the <em class="keyword">Listen</em> method returns <em class="keyword">False</em>,
							but the call does not raise an error.</p>
						<p>When the <em class="glossary">Listening mode</em> is active and the user switches to a character that
							uses a different speech recognition engine, Double Agent switches to and activates that engine and triggers
							a <em class="keyword">ListenComplete</em> and then a <em class="keyword">ListenStart</em> event. If
							the activated character does not have an available speech recognition engine (because one is not installed
							or none match the activated character's <em class="keyword">LanguageID</em> setting), Double Agent will
							trigger the <em class="keyword">ListenComplete</em> event for the previously activated character and
							passes back a value in the <em class="token">Cause</em> parameter. However, Double Agent does not generate
							<em class="keyword">ListenStart</em> or <em class="keyword">ListenComplete</em> events for the clients
							that do not have speech recognition support.</p>
						<p>If a client successfully calls the <em class="keyword">Listen</em> method and a character without speech
							recognition engine support becomes <a class="see" href="Gen_Input.htm#Active">input-active</a> before
							the <em class="glossary">Listening mode</em> time-out completes, and then the user switches back to
							the character of the original client, Double Agent will generate a <em class="keyword">ListenStart</em>
							event for that client.</p>
						<p>If the <a class="see" href="Gen_Input.htm#Active">input-active</a> client switches speech recognition
							engines by changing <em class="keyword">SRModeID</em> while in <em class="glossary">Listening mode</em>,
							Double Agent switches to and activates that engine without re-triggering the <em class="keyword">ListenStart</em>
							event. However, if the specified engine is not available, the call fails (raises an error) and Double
							Agent also calls the <em class="keyword">ListenComplete</em> event.</p>
					</dd>
				</dl>
			</td>
		</tr>
	</table>
</body>
</html>
